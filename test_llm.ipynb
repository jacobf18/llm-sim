{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we test 2 capabilities of LLMs:\n",
    "\n",
    "1. **Recognition**: Can an LLM recognize that a state-action trajectory is different than what is expected?\n",
    "2. **Rectification**: Can an LLM update simulation code to match the expected state-action trajectory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard packages\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "import multiprocessing\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition\n",
    "\n",
    "We will test whether an LLM can recognize that a state-action trajectory is different than what is expected and determine the source of the difference. We will use the Mountain Car environment from Gymnasium. The \"external\" environment will have a different gravity level than the \"internal\" environment. The LLM will see both state-action trajectories and determine what the source of the difference is. This is clearly a difficult problem computationally since the source of the difference is not directly observable and there can be many different sources of the difference.\n",
    "\n",
    "We will try to start with the easiest possible test: Match the action sequence of the external environment to the internal environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2.5:3B\",\n",
    "    temperature=0.5,\n",
    "    n_gpu_layers=-1,\n",
    "    n_threads=multiprocessing.cpu_count() - 1,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# resp = llm.invoke(\"Write a Python program that generates the first 10 numbers in the Fibonacci sequence. Include only the code. Any comments or explanations should be removed.\")\n",
    "# print(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLMs tend to code better when told to output entire code snippets as opposed to individual lines. This is because the model can better understand the context of the code. Perhaps we can use chain-of-thought with entire code snippets as one part of the chain and then utilize standard git tools to determine changes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
